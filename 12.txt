                                       TIME          COMPLEXITY
 
Time Complexity is a way to describe how the runtime of an algorithm increases with the size of the input (usually denoted as n).



ğŸ“Œ In Simple Words:

Time complexity tells you:

    â€œHow many operations will the algorithm perform as input size increases?â€

It does not measure actual time (like seconds), but rather how fast the number of steps grows.



ğŸ” Why Is It Important?

    Helps you understand how efficient your algorithm is.

    Lets you compare two solutions without running them.



ğŸ§  What is Big O Notation?

Big O notation is a mathematical way to describe the upper bound (worst-case) of how an algorithm's time or space requirement grows with input size (n).



ğŸ’¡ Why Use Big O?

    To analyze performance of algorithms.

    To compare efficiency regardless of hardware.

    Focuses on growth rate, not actual execution time.



ğŸ“Œ In Simple Words:

    Big O tells us how the runtime or memory grows as the input size increases â€” ignoring constants and small terms.




ğŸ§ª Common Big O Notations:
Big O   	                Name	                                What It Means

O(1)	                    Constant	                            Always takes same time

O(log n)	                Logarithmic	                            Very efficient â€“ like binary search

O(n)	                    Linear	                                Time grows proportionally with input

O(n log n)	                Linearithmic	                        Common in fast sorting algorithms

O(nÂ²)	                    Quadratic	                            Slows down with nested loops

O(2^n)	                    Exponential	                            Very slow for large input

O(n!)	                    Factorial	                            Impractical for big inputs




âœ… Steps to Calculate Time Complexity:

ğŸ”¹ 1. Count the number of times a statement runs

Start from the outermost block (loop/function) and move inward.


ğŸ”¹ 2. Ignore constants and non-dominant terms

When you get multiple terms like O(n + nÂ²), keep only the dominant one (O(nÂ²) here).




ğŸ“˜ Examples

ğŸ”¸ Example 1: Single Loop
for (int i = 0; i < n; i++) {
    cout << i;
}
ğŸ” Runs n times â†’ O(n)


ğŸ”¸ Example 2: Nested Loops
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        cout << i << j;
    }
}
ğŸ” Outer loop runs n times, and for each, inner runs n â†’ total = n Ã— n = O(nÂ²)


ğŸ”¸ Example 3: Logarithmic
while (n > 1) {
    n = n / 2;
}
ğŸ” Every loop halves n, so it runs logâ‚‚(n) times â†’ O(log n)


ğŸ”¸ Example 4: Combined Loops
for (int i = 0; i < n; i++) cout << i;       // O(n)
for (int j = 0; j < n; j++) cout << j;       // O(n)
ğŸ” Total = O(n) + O(n) = O(2n) â†’ Simplifies to O(n)


ğŸ”¸ Example 5: Function Calls
int func(int n) {
    if (n <= 1) return 1;
    return func(n - 1) + func(n - 2);
}
ğŸ” This is Fibonacci recursion â†’ Time Complexity = O(2â¿)


ğŸ§  Tip:
Use these rough rules:
Pattern            	            Time Complexity
Single for loop	                O(n)
Nested for loops	            O(nÂ²)
Loop with n/2, n/4...	        O(log n)
Recursive + two calls	        O(2â¿)





ğŸ”° Main Asymptotic Notations:
Notation	                    Represents	                         Meaning
Big O (O)	                    Worst-case scenario	                 Max number of operations
Big Omega (Î©)	                Best-case scenario	                 Minimum number of operations
Big Theta (Î˜)	                Average / tight bound                Exact number of operations in all cases
Little o (o)	                Strict upper bound	                 Grows slower than another function
Little omega (Ï‰)                Strict lower bound	                 Grows faster than another function



âœ… 1. Big O (O) â€“ Upper Bound

    Tells us the maximum time an algorithm can take.

ğŸ“Œ Example:
For linear search:

for (int i = 0; i < n; i++) if (arr[i] == target) return i;

    Worst-case: O(n) (target is last or not present)




âœ… 2. Big Omega (Î©) â€“ Lower Bound

    Tells us the minimum time an algorithm can take.

ğŸ“Œ Example:

    Best-case: Î©(1) (target is first element)




âœ… 3. Big Theta (Î˜) â€“ Tight Bound

    When best-case = worst-case = average-case, it's Î˜(f(n))

ğŸ“Œ Example:

    Always loops n times â†’ Î˜(n)




âœ… 4. Little o (o) â€“ Strictly Less

    Means algorithm grows slower than a function, but not equal.

ğŸ“Œ If T(n) = 2n, then:

    T(n) âˆˆ o(nÂ²)

    (i.e., grows slower than nÂ², but not equal)




âœ… 5. Little omega (Ï‰) â€“ Strictly Greater

    Means algorithm grows faster than a function, but not equal.

ğŸ“Œ If T(n) = n log n, then:

    T(n) âˆˆ Ï‰(n)

    (i.e., grows faster than n, but not equal)





ğŸ§  Summary Diagram:

O(f(n))      â†’ upper bound (â‰¤)
Î©(f(n))      â†’ lower bound (â‰¥)
Î˜(f(n))      â†’ tight bound ( = )
o(f(n))      â†’ strictly less ( < )
Ï‰(f(n))      â†’ strictly more ( > )





                             SPACE                  COMPLEXCITY



ğŸ§  What is Space Complexity?

Space Complexity is the total amount of memory or space your algorithm or program needs to run, relative to input size n.
ğŸ§¾ Formula (General Idea):

    Space Complexity = Auxiliary Space + Input Space

    ğŸ”¹ Auxiliary Space: Extra space the algorithm uses (temporary arrays, recursion stack, variables, etc.)

    ğŸ”¹ Input Space: Memory taken by input itself


âœ… Example 1: Constant Space

int sum(int* arr, int n) {
    int total = 0;
    for (int i = 0; i < n; i++) {
        total += arr[i];
    }
    return total;
}

    ğŸ§  Space Complexity: O(1)
    No extra memory is used â€” only one variable (total)



âœ… Example 2: Linear Space

int* copyArray(int* arr, int n) {
    int* newArr = new int[n];
    for (int i = 0; i < n; i++) {
        newArr[i] = arr[i];
    }
    return newArr;
}

    ğŸ§  Space Complexity: O(n)
    Extra array of size n is created



âœ… Example 3: Recursive Function

int factorial(int n) {
    if (n == 0) return 1;
    return n * factorial(n - 1);
}

    ğŸ§  Space Complexity: O(n)
    Because of the recursion stack (calls n times)




âš ï¸ Important Notes:
Factor                	                 Counts in Space Complexity?
Input array itself	                     âœ… Yes (if counted)
Temporary variable s	                 âœ… Yes
Recursion stack memory	                 âœ… Yes
Output	                                 ğŸš« No (unless explicitly asked)



ğŸ“Œ Why It Matters?

    Helps you understand memory usage, which is critical for:

        Embedded systems (low memory)

        Large-scale apps (millions of data points)

        Competitive coding (memory limits)



ğŸ“˜ Common Space Complexities (Big O)
Big O Notation	              Description	                            Example Algorithm or Code

O(1)	                      Constant Space (no extra memory)	        Simple loops with a few variables (e.g., sum of array)
O(log n)	                  Logarithmic Space	                        Binary recursion without storing all results
O(n)	                      Linear Space	                            Storing results in array or recursion with depth n
O(n log n)	                  Log-linear Space	                        Merge Sort (due to merging arrays)
O(n^2)	                      Quadratic Space	                        2D matrices, DP tables like Floyd-Warshall
O(2^n)	                      Exponential Space	                        Recursive solutions with branching (e.g., subset generation)
O(n!)	                      Factorial Space	                        Permutation generation (e.g., backtracking all permutations)